# [ActivationFunction](/Activation%20Function/activation_function.py)

- [ActivationFunction](#activationfunction)
  - [Sigmoid Function](#sigmoid-function)
  - [TanH Function](#tanh-function)
  - [ReLU Function](#relu-function)
  - [LeakyReLU function](#leakyrelu-function)
  - [Softmax function](#softmax-function)

## Sigmoid Function

## TanH Function

## ReLU Function

## LeakyReLU function

## Softmax function

Range of soft function is from 0 to 1.The Output will be probability density function.

The Softmax transform output into a valid probability function.
Softmax is used as activation of final output layer in classification problem.
