# [Optimzation Algorithms](#optimisation-algorithms)

> Optimisation algorithms are used to update weight and biases in neural network to reduce the loss function.

[Loss Function :](#loss-function) The function that is used to compute the error known as the Loss Function. >>> Mean Square Error

- [Optimzation Algorithms](#optimzation-algorithms)
  - [Constant Learning Rate Algorithms](#constant-learning-rate-algorithms)
    - [Gradient Descent](#gradient-descent)
    - [Stochastic Gradient Descent](#stochastic-gradient-descent)
    - [Mini-Batch Gradient Descent](#mini-batch-gradient-descent)
  - [Adaptive Learning Algorithms](#adaptive-learning-algorithms)
    - [Adagard](#adagard)
    - [Adadelta](#adadelta)
    - [RMSprop](#rmsprop)
    - [Adam](#adam)

## [Constant Learning Rate Algorithms](#constant-learning-rate-algorithms)

### [Gradient Descent](/Loss%20Function/GradientDescent.py)

Gradient descent is one of the most widely and commonly used machine learning algorithm.Gradient Descent known as mother of all other Optimzation-Algorithm.
Gradient Descent is an iterative optimization algorithm to find the minimum of the [Loss function](#loss-function)

For Calculaye gradient, we need to calculate derivatives of loss w.r.t the weight and the bias.

### [Stochastic Gradient Descent](/)

### [Mini-Batch Gradient Descent](/)

## [Adaptive Learning Algorithms](/Loss%20Function/Adaptive%20Learning%20Algorithm.py)

### [Adagard](/Loss%20Function/Adaptive%20Learning%20Algorithm.py)

### [Adadelta](/Loss%20Function/Adaptive%20Learning%20Algorithm.py)

### [RMSprop](/Loss%20Function/Adaptive%20Learning%20Algorithm.py)

### [Adam](/Loss%20Function/Adaptive%20Learning%20Algorithm.py)
